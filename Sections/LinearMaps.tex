\documentclass[../AlgebraQualSolutions.tex]{subfiles}

\begin{document}

\subsection{Linear Maps}

	\begin{prob}{(F17.LA2, F13.LA1)}{F13.LA1}
		Let $T: \R^3 \to \R^3$ be a linear transformation. Prove the following:
		\begin{enumerate}[(a)]
			\item $T$ has a one-dimensional invariant subspace.
			\item $T$ has a two-dimensional invariant subspace.
		\end{enumerate}
	
		\emph{ Do not use the existence of canonical forms for matrices.}
	\end{prob}

	\begin{proof}
		Let $p(x)$ be the characteristic polynomial of $T$. Since $T: \R^3 \to \R^3$, it follows that $\deg(p) = 3$. Therefore $p$ has some real root, say $\lambda$. Then there exists a degree 2 polynomial $q(x) = x^2 + ax + b$ such that $p(x) = (x-\lambda)q(x)$.\\

		Any real root of $p$ corresponds to an eigenvalue of $T$. Therefore $\lambda$ is an eigenvalue of $T$ with some associated eigenvector, say $v$. The subspace $\rm{span}\{v\}$ is clearly a one-dimensional $T$ invariant subspace.\\

		Let $m(x)$ be the minimal polynomial of $T$. Then $m(x) = (x-\lambda)f(x)$ for some polynomial $f$ of degree at most 2. By definition, $m$ is the polynomial of smallest degree so that $m(T) = 0$. Define $K = \ker(f(T))$. If $\dim(K) = 3$, then $K = \R^3$ and $f(T) = 0$. This contradicts the minimality of $m$ and therefore either $\dim(K) = 1$ or $\dim(K) = 2$.\\

		If $\dim(K) = 1$, the rank-nullity theorem implies that $\range(f(T)) = 2$. For any $v \in \R^3$,
			\[(T - \lambda I)f(T)v = 0\]
		since $m(T) = 0$. Therefore each element in $\range(f(T))$ is an eigenvector associated with the eigenvalue $\lambda$. This means that $\range(f(T))$ is a two-dimensional $T$-invariant subspace.\\

		Now assume that $\dim(K) = 2$. For any $v \in K$, $f(T)v = 0$. Then,
			\[f(T)(Tv) = T(f(T)v) = T(0) = 0\]
		implying that $Tv \in \ker(f(T)) = K$. That is, $K$ is a two-dimensional $T$-invariant subspace.
	\end{proof}

	\begin{prob}{(S18.LA3)}{S18.LA3}
		Let $T: V \to W$ be a linear transformation between two finite dimensional vector spaces. Prove that there exist bases for $V$ and $W$ such that the matrix representation for $T$ with respect to these bases has an identity matrix in the top left corner and all other entries equal to zero.
	\end{prob}

	\begin{proof}
		Let $T: V \to W$ be a linear map. Let $v_1,\ldots, v_n$ be a basis for $\null(T)$. Extend this list to a basis $v_1,\ldots, v_n,u_1,\ldots,u_m$ for $V$.

		\begin{claim}
			The list $Tu_1,\ldots, Tu_m$ is a basis for $\range(T)$.

			\begin{proof}
				Since $v_1,\ldots, v_n,u_1,\ldots, u_m$ form a basis for $V$ any $Tv$ can be written as 
					\[Tv = T(a_1v_1 + \cdots + a_nv_n + b_1u_1 + \cdots + b_mu_m).\]
				But each $Tv_k = 0$ and therefore
					\[Tv = b_1Tu_1 + \cdots + b_mTu_m.\]
				That is, $Tu_1,\ldots, Tu_m$ span $\range(T)$. It remains to show that $Tu_1,\ldots, Tu_m$ are linearly independent. Assume that 
					\[a_1Tu_1 + \cdots + a_mTu_m = 0.\]
				By linearity, this implies that
					\[T(a_1u_1 + \cdots + a_mu_m) = 0\]
				and so $a_1u_1 + \cdots + a_mu_m \in \null(T)$. Find $b_1,\ldots, b_n$ so that
					\[a_1u_1 + \cdots + a_mu_m = b_1v_1 + \cdots + b_nv_n\]
				or equivalently,
				\[a_1u_1 + \cdots + a_mu_m - b_1v_1 - \cdots - b_nv_n = 0.\]
				But $u_1,\ldots,u_m,v_1,\ldots,v_n$ is a basis for $V$ and thus linearly independent. That is, $a_1 = \cdots = a_m = b_1 = \cdots = b_n = 0$.
			\end{proof}
		\end{claim}

		Reorder the list to $u_1,\ldots,u_m,v_1,\ldots, v_n$. Note that reordering the list does not change linear independence or the span. Since $Tu_k = 1\cdot Tu_k$ and $Tv_k = 0$ for each $k$, the matrix for $T$ with respect to this basis is as desired.
	\end{proof}

	\begin{prob}{S12.LA2}{S12.LA2}
		Let $V$ be a finite dimensional vector space. A linear transformation $T: V \to V$ is a projection when $T = T^2$. Prove that there exists a basis for $V$ such that the matrix for $T$ with respect to this basis is a diagonal matrix with diagonal entries all zeros or ones.
	\end{prob}

	\begin{proof}
		Since $T = T^2$, $V = \Null(T) \oplus \range(T)$ (see \ref{prob:S20.LA4}). Let $v_1,\ldots, v_n$ be a basis for $\Null(T)$ and extend this list to $v_1,\ldots, v_n, w_1,\ldots,w_m$ to obtain a basis for $V$. Then $Tw_1,\ldots, Tw_m$ is a basis for $\range(T)$. Since $V = \Null(T) \oplus \range(T)$, the list $v_1,\ldots, v_n, Tw_1,\ldots, Tw_m$ form a basis for $V$. With respect to this basis, the matrix for $T$ is as desired.
	\end{proof}

	\emph{Technically there are some more details to prove here, such as linear independence of $Tw_1,\ldots, Tw_m$, but this is all routine.}

	\begin{prob}{S20.LA4}{S20.LA4}
		Let $V$ be a finite dimensional vector space and $T: V \to V$ a linear map such that $T^2 = T$.
		\begin{enumerate}[(a)]
			\item Prove that $V = T(V) \oplus \ker(T)$.
			\item If $S: V \to V$ is another linear map and $S^2 = S$, $S(V) = T(V)$, and $\ker(S) = \ker(T)$, prove that $S = T$.
		\end{enumerate}
	\end{prob}

	\begin{proof}
		Let $v \in V$. Observe that $v = Tv + (v - Tv)$. Clearly $Tv \in T(V)$ and since $T(v - Tv) = Tv - T^2v = 0$, $v - Tv \in \ker(T)$. Therefore, $V = T(V) + \ker(T)$. To see that this sum is actually a direct sum, it suffices to show that $T(V) \cap \ker(T) = \{0\}$. Let $v \in T(V) \cap \ker(T)$. Then, $Tv = 0$ and $v = Tw$ for some $w \in V$. Together, this means that
			\[0 = Tv = T^2w = Tw = v\]
		meaning the intersection is trivial.
	\end{proof}

	\begin{proof}
		Let $v \in V$. Since $V = S(V) \oplus \ker(S)$, we may write $v = Su + w$ where $w \in \ker(S)$. But, $Su \in S(V) = T(V)$ means that there exists $u' \in V$ such that $Su = Tu'$. Also, $\ker(S) = \ker(T)$ implies that $Tw = 0$. Therefore,
			\[Sv = S(Su + w) = S^2u + Sw = Su = Tu' = T^2u' + Tw = T(Su + w) = Tv.\]
		As this holds for each $v \in V$, $S = T$.
	\end{proof}

	\begin{prob}{S20.LA3}{S20.LA3}
		Let $V$ be a finite-dimensional vector space and $S,T: V \to V$ linear transformations.
		\begin{enumerate}[(a)]
			\item Prove that $\rank{S+T} \leq \rank{S} + \rank{T}$
			\item Prove that $\rank{ST} \leq \max(\rank{S}, \rank{T})$.
		\end{enumerate}
	\end{prob}

	\begin{proof}
		Recall that for any two subspaces $U_1,U_2$ of a finite dimensional vector space $V$,
			\[\dim(U_1 + U_2) = \dim(U_1) + \dim(U_2) - \dim(U_1 \cap U_2).\]
		As the dimension is always non-negative, $\dim(U_1 + U_2) \leq \dim(U_1) + \dim(U_2)$. By the linearity of $S$ and $T$, for any $v \in V$,
			\[(S + T)v = Sv + Tv.\]
		Therefore, $\range(S + T) = \range(S) + \range(T)$. This means that
			\[\rank{S+T} = \dim(\range(S) + \range(T)) \leq \dim(\range(S)) + \dim(\range(T)) = \rank{S} + \rank{T},\]
		as desired.
	\end{proof}

	\begin{proof}
		For any $v \in V$, $STv = S(Tv)$ and therefore $\range(ST) \sq \range(S)$. Since the dimension of a subspace is bounded above by the dimension of the larger space,
			\[\rank{ST} = \dim(\range(ST)) \leq \dim(\range(S)) = \rank{S} \leq \max\{\rank{S},\rank{T}\}.\]
	\end{proof}

	\begin{prob}{S19.LA4}{S19.LA4}
		Let $V$ be the vector space of all $2 \times 2$ matrices and let 
			\[A = \begin{pmatrix} 2 & 0\\ 0 & 3 \end{pmatrix}. \]
		Let $T: V \to V$ be given by $T(B) = AB + BA$.

		\begin{enumerate}[(a)]
			\item Prove that $T$ is a linear map.
			\item Compute $T(B)$ when $B = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$.
			\item Find the eigenvalues and corresponding eigenspaces of $T$.
		\end{enumerate}
	\end{prob}

	\begin{proof}
		Let $B,C \in V$ and $\alpha \in \R$. Following from properties of matrix multiplication and addition,
			\[T(B + C) = A(B+C) + (B+C)A = AB + AC + BA + CA = T(B) + T(C).\]
		Likewise,
			\[T(\alpha B) = A(\alpha B) + (\alpha B)A = \alpha(AB +BA) = \alpha T(B).\]
	\end{proof}

	\begin{solution}
		\[T\left(\begin{pmatrix} a & b \\ c & d \end{pmatrix}\right) = \begin{pmatrix} 2a & 2b \\ 3c & 3d \end{pmatrix}.\]
	\end{solution}

	\begin{solution}
		Notice that $V$ is dimension 4 and so the sum of the dimensions  of the eigenspaces is at most 4. Upon inspection, we find that $\lambda = 2$ is an eigenvalue with an eigenspace basis given by $\left\{ \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} \right\}$. Similarly, $\lambda = 3$ is an eigenvalue with eigenspace basis given by $\left\{ \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix} \right\}$. 
	\end{solution}

	\begin{prob}{F16.LA1, F14.LA1}{F14.LA1}
		Suppose that $T: U \to V$ is a linear transformation with $U$ and $V$ both finite-dimensional vector spaces. Prove that
			\[\dim(\ker(T)) + \dim(\rm{range}(T)) = \dim(U).\]
	\end{prob}

	\begin{proof}
		Suppose that $\dim(U)$ and $\dim(V)$ are both finite. In particular, any subspace of $U$ and $V$ is also finite dimensional. Let $\{v_1,\ldots, v_m\}$ be a basis for $\ker(T) \sq U$. Extend this collection to a basis $\beta = \{v_1,\ldots, v_m, w_1,\ldots, w_n\}$ of $U$. With this chosen basis, it follows that $\dim(\ker(T)) = m$ and $\dim(U) = m + n$. Thus we must show that $\dim(\range(T)) = n$.\\

		Let $Tu$ be an arbitrary element of $\range(T)$. Since $\beta$ is a basis for $U$, there exist scalars $a_1,\ldots, a_m$ and $b_1,\ldots,b_n$ such that
			\[u = a_1v_1 + \cdots + a_mv_m + b_1w_1 + \cdots + b_nw_n. \]
		By the linearity of $T$, this means that
			\[Tu = a_1Tv_1 + \cdots + a_mTv_m + b_1Tw_1 + \cdots + b_nTw_n = b_1Tw_1 + \cdots + b_nTw_n.\]
		The second equality follows as each $v_i \in \ker(T)$ and therefore $a_iTv_i = 0$. This means that $\{Tw_1,\ldots,Tw_n\}$ span $\range(T)$.\\

		To see that $\{Tw_1,\ldots,Tw_n\}$ is a linearly independent set, suppose that
			\[c_1Tw_1 + \cdots + c_nTw_n = 0.\]
		By linearity,
			\[T(c_1w_1 + \cdots + c_nw_n) = 0\]
		and so $c_1w_1 + \cdots + c_nw_n \in \ker(T)$. Choose scalars $d_1,\ldots, d_m$ such that
			\[c_1w_1 + \cdots + c_nw_n = d_1v_1 + \cdots + d_mv_m.\]
		Rearranging,
			\[c_1w_1 + \cdots + c_nw_n - d_1v_1 - \cdots - d_mv_m = 0.\]
		But, $\beta$ is a basis and so the collection of elements in $\beta$ is linearly independent. That is, $c_1 = \cdots = c_n = d_1 = \cdots = d_m = 0$. Therefore, $\{Tw_1,\ldots,Tw_n\}$ are linearly independent. As this is a linearly independent spanning set for $\range(T)$, $\dim(\range(T)) = n$ as desired.
	\end{proof}

	\begin{prob}{F12.LA2}{F12.LA2}
	Suppose that $V = X \oplus Y$ and define the projection $V \to X$ by $\alpha(v) = x$ where $v = x+y$.
	\begin{enumerate}[(a)]
	\item Prove that a necessary and sufficient condition for an endomorphism $T: V \to V$ to be a projection is that $T^2 = T$. Identify $X$ and $Y$ in the case that this condition is satisfied.
	\item Prove that projections $T_1$ and $T_2$ have the same range if and only if $T_1T_2 = T_2$ and $T_2T_1 = T_1$.
	\end{enumerate}
	\end{prob}

	\begin{proof}
		Suppose first that $T: V \to V$ is a projection map. That is, for any $v = x + y \in X \oplus Y = V$, $Tv = x$. Then,
			\[T^2v = T(T(x+y)) = T(x) = x = T(x+y) = Tv\]
		and therefore $T^2 = T$.\\

		Now assume that $T^2=T$. Let $v  \in V$. Observe that $v = (v - Tv) + Tv$. Applying $T$ to both sides yields the following set of equalities:
			\[Tv = T(v-Tv) + T^2v = T(v-Tv) + Tv\]
		implying that $T(v-Tv) =0$ and therefore $v-Tv \in \range(T)$. Because $Tv \in \range(T)$, $v \in \ns(T) + \range(T)$.\\
		
		To show that $\ns(T) + \range(T)$ is a direct sum, suppose that $0 = x + Tu \in \ns(T) + \range(T)$. Then,	
			\[0 = T(0) = Tx + T^2u = 0 + Tu = Tu \]
		and therefore $x = 0$ as well. Since $x = Tu = 0$, $\ns(T) + \range(T)$ is a direct sum. That is, $V = \range(T) \oplus \ns(T)$.\\

		Since $Tv \in \range(T)$ for any $v \in V$, $T$ is indeed a projection.
	\end{proof}

	\begin{proof}
		Suppose that $\range(T_1) = W = \range(T_2)$ with both $T_1$ and $T_2$ projections. Let $v \in V$ and suppose that $v = w + w'$ where $w \in W$. Then,
			\[T_1T_2v = T_1w = w = T_2v\]
		and similarly,
			\[T_2T_1v = T_2w = w = T_1v.\]
		Assume now that $T_1T_2 = T_2$ and $T_2T_1 = T_1$. Let $v \in V$ and consider $T_1v \in \range(T_1)$. Then,
			\[T_1v = T_2T_1v \in \range(T_2).\]
		Similarly,
			\[T_2v = T_1T_2v \in \range(T_1) \]
		implying that $\range(T_1) = \range(T_2)$.
	\end{proof}



\end{document}
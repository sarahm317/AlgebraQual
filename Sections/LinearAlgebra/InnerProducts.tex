\documentclass[../../AlgebraQualSolutions.tex]{subfiles}

\begin{document}

\subsection{Inner Products}

\begin{prob}{(F04.L2)}{F04.L2}
    Let $V$ be a finite dimensional inner product space over $\R$ and $W \sq V$ a subspace. Show that:

    \begin{enumerate}[(a)]
        \item $V = W + W^\perp$.
        \item $W = (W^\perp)^\perp$.
    \end{enumerate}
\end{prob}

\begin{proof}
    Let $w_1,\ldots, w_m$ be a basis for $W$. By applying the Gram-Schmidt process (see \ref{prob:S18.LA2}), we may assume this basis is orthonormal. Extend this list to a basis $w_1,\ldots, w_m, v_1, \ldots, v_n$ of $V$, again without loss of generality assuming the basis is orthonormal.

    \begin{claim}
        The list $v_1, \ldots, v_n$ is a basis for $W^\perp$. 

        \begin{proof}
            By construction, the list $v_1, \ldots, v_n$ is linearly independent and thus it suffices to prove that this list spans $W^\perp$. Let $u \in W^\perp$. Since $w_1,\ldots, w_m, v_1, \ldots, v_n$ is a basis of $V$, write
                \[u = a_1w_1 + \cdots + a_m w_m  + b_1v_1 + \cdots + b_n v_n.\]
            Since $u \in W^\perp$, for each $j = 1,\ldots, m$, $\ip{u}{w_j} = 0$. But, the list $w_1,\ldots, w_m, v_1, \ldots, v_n$ is orthonormal and therefore,
                \[0 = \ip{u}{w_j} = a_1 \ip{w_1}{w_j} + \cdots + a_m \ip{w_m}{w_j} + b_1 \ip{v_1}{w_j} + \cdots + b_n \ip{v_n}{w_j} = a_j\ip{w_j}{w_j} = a_j.\]
            That is,
                \[u = b_1v_1 + \cdots + b_n v_n,\]
            proving that $W^\perp = \textrm{span}\{v_1, \ldots, v_n\}$.
        \end{proof}
    \end{claim}
    From the claim, it follows that $V = W + W^\perp$. By construction, $W \cap \textrm{span}\{v_1, \ldots, v_n\} = \{0\}$ and thus $V = W + W^\perp = W \oplus W^\perp$, as desired.
\end{proof}

\begin{proof}
    \textcolor{red}{Let $w \in W$ be arbitrary. For each $u \in W^\perp$, $\ip{u}{w} = 0$. Therefore, $w \in (W^\perp)^\perp$. On the other hand, suppose that $w \in (W^\perp)^\perp$. Then $\ip{w}{u} = 0$ for each $u \in W^\perp$.}
\end{proof}


\begin{prob}{(S18.LA2)}{S18.LA2}
    Let $V = \R^n$ with the Euclidean inner product. Let $U \sq V$ be a subspace.
    \begin{enumerate}[(a)]
        \item Prove that $U$ has an orthonormal basis.
        \item Find an orthonormal basis for the space of $(1,1,0)$ and $(1,2,3)$ inside $\R^3$.
    \end{enumerate}
\end{prob}

\begin{proof}
    Any subspace has a basis. Therefore, let $u_1,\ldots, u_m$ be a basis for $U$. Let $v_1 = u_1$. Next define
        \[v_2 = u_2 - \frac{\ip{v_1}{u_2}}{\norm[v_1]}v_1.\]
    Observe:
        \[\ip{v_1}{v_2} = \ip{v_1}{u_2} - \frac{\ip{v_1}{u_2}}{\ip{v_1}{v_1}}\ip{v_1}{v_1} = 0\]
    following directly from the linearity properties of the inner product. For each $j = 2,\ldots,m$, define
        \[v_j = u_j - \sum_{k=1}^{j-1}\frac{\ip{v_k}{u_j}}{\norm[v_k]}v_k.\]

        \begin{claim}
            For any $1 \leq i < j \leq m$, $\ip{v_i}{v_j} = 0$.

            \begin{proof}
                For the base case, notice that $\ip{v_1}{v_2} = 0$. Assume now that whenever $1 \leq i < j < k$, $\ip{v_i}{v_j} = 0$. Now for any $i = 1, \ldots, k-1$,
                    \begin{align*}
                        \ip{v_k}{v_i} &= \ip{u_k - \sum_{j=1}^{i-1} \frac{\ip{v_j}{u_k}}{\norm[v_j]}v_j}{v_i}\\
                        &= \ip{u_k}{v_i} - \sum_{j=1}^{i-1} \frac{\ip{v_j}{u_k}}{\norm[v_j]}\ip{v_j}{v_i}\\
                        &= \ip{u_k}{v_i} - \frac{\ip{v_i}{u_k}}{\norm[v_i]}\ip{v_i}{v_i} \label{eq:IH}\\
                        &= 0
                    \end{align*}
                Line (\ref{eq:IH}) follows from the inductive hypothesis since $i, j < k$.
            \end{proof}
        \end{claim}

        \begin{claim}
            The list $v_1, \ldots, v_n$ is linearly independent.

            \begin{proof}
                Assume that
                \[a_1v_1 + \cdots + a_nv_n = 0\]
                for some constants $a_1,\ldots, a_n$. Notice that each $v_j$ is a linear combination of $u_1,\ldots, u_j$. By rewriting the equation in terms of the $u_j$ and using the fact that $u_1,\ldots, u_n$ are linearly independent, it follows  that $a_1 = \cdots = a_n = 0$.
            \end{proof}
        \end{claim}

    Next define $e_j = \frac{1}{\norm[v_j]}v_j$ for each $j = 1,\ldots, m$. Then each $e_j$ has norm 1, the list $e_1,\ldots, e_m$ is pair-wise orthogonal, and is linearly independent. A linearly independent list of $n$ vectors in an $n$-dimensional space forms a basis, proving that $e_1,\ldots, e_n$ is as desired.
\end{proof}

\begin{solution}
    Let $u_1 = (1,1,0)$ and $u_2 = (1,2,3)$. Using the formulas as defined in the previous proof, let $v_1 = u_1$ and $v_2 = u_2 - \frac{\ip{v_1}{u_2}}{\norm[v_1]}v_1 = (-1/2,1/2,3/2)$. Normalizing each of these vectors, we obtain an orthonormal basis:

        \[\left\{ \begin{pmatrix} 1/\sqrt2\\ 1/\sqrt 2\\ 0 \end{pmatrix}, \begin{pmatrix}-\sqrt{11}/8 \\ \sqrt{11}/8\\ 3\sqrt{11}/8 \end{pmatrix} \right\}.\]
\end{solution}

\begin{prob}{S19.LA4}{S19.LA4}
    Let $V$ be a finite dimensional inner product space over $\R$. If $A$ and $B$ are subspaces of $V$, prove that $(A+B)^\perp = A^\perp \cap B^\perp$.
\end{prob}

\begin{proof}
    Suppose that $v \in (A+B)^\perp$ and let $a \in A$ be arbitrary. Then, $a + 0 \in A+B$ and therefore,
        \[\ip{a}{v} = \ip{a+0}{v} = 0\]
    since $\ip{a + b}{v} = 0$ for all $a + b \in A + B$. Similarly, $\ip{b}{v} = 0$ for any $b \in B$. Thus $v \in A^\perp \cap B^\perp$.\\

    Now assume that $v \in A^\perp \cap B^\perp$. Let $a + b \in A + B$ be arbitrary. By the linearity of the inner product,
        \[\ip{a + b}{v} = \ip{a}{v} + \ip{b}{v} = 0 + 0 = 0\]
    proving that $v \in (A+B)^\perp$.
\end{proof}

\begin{prob}{F12.LA1}{F12.LA1}
    Let $V$ be the vector space of real $n \times n$ matrices. Show that 	
        \[\langle A, B \rangle = n \tr(AB) - \tr(A)\tr(B) \]
    defines a symmetric bilinear form on $V$.
    \begin{enumerate}[(a)]
    \item Prove that $\langle,\rangle$ is singular.
    \item Prove that the restriction of $\langle,\rangle$ to the subspace $W$ of symmetric matrices with 0 trace is positive definite.
    \end{enumerate}
    \end{prob}
    
    \begin{proof}
        Note that for any $n \times n$ real matrices $A$ and $B$, $\tr(AB) = \tr(BA)$ and $\tr(A+B) = \tr(A) + \tr(B)$. Furthermore, whenever $\alpha \in \R$, $\tr(\alpha A) = \alpha \tr(A)$. Therefore,
            \[\ip{A}{B} = n \tr(AB) - \tr(A)\tr(B) = n \tr(BA) - \tr(B)\tr(A) = \ip{B}{A}\]
        meaning that $\ip{\cdot}{\cdot}$ is symmetric. Since
            \begin{align*}
                \ip{A}{\alpha B + \beta C} &= n\tr(A(\alpha B + \beta C)) - \tr(A) \tr(\alpha B + \beta C)\\
                &= n\tr(\alpha AB + \beta AC) - \tr(A) \tr(\alpha B + \beta C)\\
                &= \alpha n \tr(AB) + \beta n \tr(AC) - \alpha \tr(A)\tr(B) - \beta \tr(A)\tr(C)\\
                &= \alpha \ip{A}{B} + \beta\ip{A}{C}
            \end{align*}
        it follows that $\ip{\cdot}{\cdot}$ is a bilinear form.
    \end{proof}
    
    \begin{proof}
        Let $I$ be the $n \times n$ identity matrix and observe that for any nonzero $n \times n$ matrix $B$,
            \[\ip{I}{B} = n\tr(IB) - \tr(I)\tr(B) = n\tr(B) - n \tr(B) = 0.\]
        Since $I \neq 0$ and $B$ was arbitrary, $\ip{\cdot}{\cdot}$ is singular.
    \end{proof}
    
    \begin{proof}
        Let $W = \{A_{n\times n}: A \textrm{ is symmetric  and } \tr(A) = 0\}.$ Then,
            \[\ip{A,A} = n\tr(A^2) - \tr(A)\tr(A) = n\tr(A^2)\]
        whenever $A \in W$. Suppose that $A = (a_{ij})$ where $a_{ij}$ denotes the $(i,j)$ entry of $A$ with $1\leq i,j \leq n$. Because $A$ is symmetric, $a_{ij} = a_{ji}$. Therefore the $(i,i)$ entry of $A^2$ is
            \[b_i = a_{i1}a_{1i} + \cdots + a_{in}a_{ni} = \sum_{j=1}^n a_{ij}^2. \]
        Thus, the trace of $A^2$ can be written as $\tr(A^2) = \sum_{i=1}^n b_i = \sum_{i=1}^n \sum_{j=1}^n a_{ij}^2$. As each term in the sum is at least zero, $\ip{A}{A} = \tr(A^2) \geq 0$. If $\ip{A}{A} = 0$, then $\sum_{i=1}^n \sum_{j=1}^n a_{ij}^2 = \tr(A^2) = 0$. As each term in the sum is non-negative, each term must then equal zero. That is, $a_ij = 0$ for $i,j = 1, \ldots, n$ and therefore $A = 0$.
    \end{proof}
    
    \begin{prob}{F14}{F14.LA3}
        Let $U$ be a real inner-product space and $T: U \to U$ a linear operator on $U$. Prove that $T$ is an orthogonal linear transformation if and only if $\norm[T(u)] = \norm[u]$ for all $u \in U$.\\
    
        \emph{$T$ is an orthogonal linear transformation if $\ip{u}{v} = \ip{Tu}{Tv}$}.
    \end{prob}
    
    \begin{proof}
        Assume first that $T$ is an orthogonal linear transformation. For any $u \in U$,
            \[\ip{u}{u} = \norm[u]^2 = \norm[Tu]^2 = \ip{Tu,Tu}.\]
        Conversely, assume that $\norm[T(u)] = \norm[u]$ for all $u \in U$. Notice that this implies $\ip{Tu}{Tu} = \ip{u}{u}$ for each $u \in U$. Let $x, y \in U$ be arbitrary. Then,
            \[\ip{x+y}{x+y} = \ip{x}{x} + 2\ip{x}{y} + \ip{y}{y} = \ip{Tx}{Tx} + 2\ip{x}{y} + \ip{Ty}{Ty}.\]
        On the other hand,
            \[\ip{T(x+y)}{T(x+y)} = \ip{Tx + Ty}{Tx + Ty} = \ip{Tx}{Tx} + 2\ip{Tx}{Ty} + \ip{Ty}{Ty}.\]
        Since $\norm[x + y] = \norm[T(x+y)]$, we may equate the right sides of the above equations to find that $\ip{Tx}{Ty} = \ip{x}{y}$.
    \end{proof}
\end{document}